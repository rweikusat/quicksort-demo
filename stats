These statistics where generated by running

make benchmark FINAL=1

in a pristine repository (FINAL=1 requests compiling with optimization
turned on). A benchmark run consists of running one of the two sorting
programs 4000 times with a random permutation of the numbers from 1 to
16384 as input (created via shuf), as implemented by scripts/benchmark.

The results were:

quicksort algorithm
-------------------
comparisons

avg 367599 (1.60 nlogn)
std dev 8503
max 432185
min 342129
median 366264

swaps

avg 53651 (0.23 nlogn)
std dev 318
max 54782
min 51957
median 53684

qs-faster algorithm
-------------------
comparisons

avg 277239 (1.21 nlogn)
std dev 8532
max 340026
min 251075
median 275632

swaps

avg 51589 (0.22 nlogn)
std dev 326
max 52814
min 50025
median 51630

The "nlogn" factors (average divided by number of elements times
log₂(number of elements), the theoretically best value) seem to
consistent for different array sizes and iteration counts.

The three way partitioning algorithm à la Sedgewick/ Knuth is thus
clearly better than the one I invented on my own although the
difference is fairly slight.
