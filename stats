These statistics where generated by running

make benchmark FINAL=1

in a pristine repository (FINAL=1 requests compiling with optimization
turned on). A benchmark run consists of running one of the two sorting
programs 4000 times with a random permutation of the numbers from 1 to
16384 as input (created via shuf), as implemented by scripts/benchmark.

The results were:

quicksort algorithm
-------------------
comparisons

avg 367584 (1.60 nlogn)
std dev 8492
max 422977
min 340545
median 366169

swaps

avg 53651 (0.23 nlogn)
std dev 319
max 54800
min 51662
median 53687


qs-faster algorithm
-------------------
comparisons

avg 323050 (1.41 nlogn)
std dev 9572
max 375537
min 291638
median 321958

swaps

avg 51597 (0.22 nlogn)
std dev 328
max 52686
min 49985
median 51635

The "nlogn" factors (average divided by number of elements times
log₂(number of elements), the theoretically best value) seem to
consistent for different array sizes and iteration counts.

The three way partitioning algorithm à la Sedgewick/ Knuth is thus
clearly better than the one I invented on my own although the
difference is fairly slight.
